import logging
import json
import re
import base64
from fastapi import FastAPI, HTTPException, APIRouter, UploadFile, File
from pydantic import BaseModel
from typing import List, Optional, Dict
from contextlib import asynccontextmanager

from src.core.model_engine import model_engine
from src.core.prompts import VISION_ANALYSIS_PROMPT

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("ğŸš€ AI Service Starting...")
    try:
        model_engine.initialize()
    except Exception as e:
        logger.error(f"âš ï¸ Model init warning: {e}")
    yield
    logger.info("ğŸ’¤ AI Service Shutting down...")

app = FastAPI(title="Modify AI Service", version="1.0.0", lifespan=lifespan)
api_router = APIRouter(prefix="/api/v1")

# --- DTO ---
class EmbedRequest(BaseModel):
    text: str

class EmbedResponse(BaseModel):
    vector: List[float]

class ImageAnalysisResponse(BaseModel):
    name: str
    category: str
    description: str
    price: int
    vector: List[float]

class PathRequest(BaseModel):
    query: str

class InternalSearchRequest(BaseModel):
    query: str
    image_b64: Optional[str] = None

class SearchProcessResponse(BaseModel):
    vector: List[float]
    reason: str

# --- Endpoints ---

@api_router.post("/embed-text", response_model=EmbedResponse)
async def embed_text(request: EmbedRequest):
    try:
        vector = model_engine.generate_embedding(request.text)
        return {"vector": vector}
    except:
        return {"vector": [0.0] * 768} 

@api_router.post("/analyze-image", response_model=ImageAnalysisResponse)
async def analyze_image(file: UploadFile = File(...)):
    filename = file.filename
    try:
        contents = await file.read()
        image_b64 = base64.b64encode(contents).decode("utf-8")
        
        prompt = VISION_ANALYSIS_PROMPT
        
        logger.info(f"ğŸ‘ï¸ Analyzing image: {filename}...")
        generated_text = model_engine.generate_with_image(prompt, image_b64)
        logger.info(f"ğŸ¤– Raw AI Response: {generated_text}")

        # [Safety Check]
        if "cannot assist" in generated_text or "I cannot" in generated_text:
            raise ValueError("AI Safety Filter Triggered")

        # [Parsing] JSON ì¶”ì¶œ
        product_data = {}
        try:
            # 1. ê°€ì¥ ë¨¼ì € ë°œê²¬ë˜ëŠ” { ... } ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r"\{[\s\S]*\}", generated_text)
            if json_match:
                clean_json = json_match.group()
                # ë§ˆí¬ë‹¤ìš´ ì œê±°
                clean_json = re.sub(r"```json|```", "", clean_json)
                product_data = json.loads(clean_json)
            else:
                # ì „ì²´ê°€ JSONì¼ ê²½ìš°
                product_data = json.loads(generated_text)
        except Exception as e:
            logger.warning(f"JSON Parsing failed: {e}. Raw: {generated_text[:50]}...")

        # [Data Validation & Fallback]
        final_name = product_data.get("name")
        # ì´ë¦„ì´ ë¹„ì—ˆê±°ë‚˜ í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ì•µë¬´ìƒˆì²˜ëŸ¼ ë”°ë¼í•œ ê²½ìš° ì²´í¬
        if not final_name or "ìƒí’ˆëª…" in final_name or "JSON" in final_name:
             final_name = f"AI ì¶”ì²œ ìƒí’ˆ ({filename.split('.')[0]})"
        
        final_desc = product_data.get("description")
        if not final_desc or len(final_desc) < 10:
            final_desc = "AIê°€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì¶”ì²œí•˜ëŠ” ìƒí’ˆì…ë‹ˆë‹¤. ë§¤ë ¥ì ì¸ ìŠ¤íƒ€ì¼ê³¼ ë›°ì–´ë‚œ í’ˆì§ˆì„ ìë‘í•©ë‹ˆë‹¤."
            
        final_cat = product_data.get("category", "Uncategorized")
        
        # ê°€ê²© ì²˜ë¦¬
        try:
            raw_price = str(product_data.get("price", 0))
            price = int(re.sub(r"[^0-9]", "", raw_price))
        except:
            price = 0

        # ë²¡í„° ìƒì„± (ê²€ìƒ‰ìš©)
        meta_text = f"{final_name} {final_cat} {final_desc}"
        vector = model_engine.generate_embedding(meta_text)

        return {
            "name": final_name,
            "category": final_cat,
            "description": final_desc,
            "price": price,
            "vector": vector
        }

    except Exception as e:
        logger.error(f"âŒ Analysis Error: {e}")
        return {
            "name": f"ë“±ë¡ëœ ìƒí’ˆ ({filename})",
            "category": "Etc",
            "description": "ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ì ëª¨ë“œì—ì„œ ì •ë³´ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.",
            "price": 0,
            "vector": [0.0] * 768
        }

@api_router.post("/llm-generate-response")
async def llm_generate(body: Dict[str, str]):
    prompt = body.get("prompt", "")
    try:
        korean_prompt = f"ì§ˆë¬¸: {prompt}\në‹µë³€ (í•œêµ­ì–´):"
        answer = model_engine.generate_text(korean_prompt)
        return {"answer": answer}
    except:
        return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

@api_router.post("/determine-path")
async def determine_path(request: PathRequest):
    return {"path": "INTERNAL"}

@api_router.post("/process-internal", response_model=SearchProcessResponse)
async def process_internal(request: InternalSearchRequest):
    query = request.query
    vector = model_engine.generate_embedding(query)
    return {"vector": vector, "reason": f"'{query}' ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤."}

@api_router.post("/process-external", response_model=SearchProcessResponse)
async def process_external(request: InternalSearchRequest):
    return await process_internal(request)

app.include_router(api_router)

@app.get("/")
def read_root():
    return {"message": "Modify AI Service is Running"}